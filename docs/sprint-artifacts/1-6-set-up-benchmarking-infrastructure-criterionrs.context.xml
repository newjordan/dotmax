<story-context id=".bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>6</storyId>
    <title>Set Up Benchmarking Infrastructure (Criterion.rs)</title>
    <status>drafted</status>
    <generatedAt>2025-11-17</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>/mnt/e/dotmax/docs/sprint-artifacts/1-6-set-up-benchmarking-infrastructure-criterionrs.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>developer for whom performance is make-or-break</asA>
    <iWant>a benchmarking system that measures and tracks performance over time</iWant>
    <soThat>I can validate &lt;50ms renders and catch regressions immediately</soThat>
    <tasks>
- Task 1: Add criterion dependency and bench configuration to Cargo.toml (AC: #1, #2)
  - Add `criterion = { version = "0.5", features = ["html_reports"] }` to [dev-dependencies]
  - Add [[bench]] section with name = "rendering" and harness = false
  - Verify cargo bench compiles (even without benchmark files yet)

- Task 2: Create benches/rendering.rs with placeholder benchmarks (AC: #3, #4, #5, #6)
  - Create benches/ directory if not exists
  - Create benches/rendering.rs file
  - Add criterion imports: use criterion::{black_box, criterion_group, criterion_main, Criterion}
  - Implement bench_braille_grid_creation placeholder (benchmark a simple allocation/vec creation)
  - Implement bench_grid_clear placeholder (benchmark clearing a vec with fill(0))
  - Implement bench_unicode_conversion placeholder (benchmark char::from_u32 conversions in 0x2800 range)
  - Add criterion_group macro to register all three benchmarks
  - Add criterion_main macro to execute benchmark group

- Task 3: Run and validate benchmarks locally (AC: #7, #8, #9, #10)
  - Run `cargo bench` and verify no compilation errors
  - Verify console output shows timing results for all three benchmarks
  - Verify target/criterion/ directory created with HTML reports
  - Open target/criterion/report/index.html to verify report structure
  - Run `cargo bench` second time to verify baseline comparison works
  - Verify criterion shows "change" metrics on second run

- Task 4: Create GitHub Actions benchmark workflow (AC: #11, #12, #13, #14)
  - Create .github/workflows/benchmark.yml file
  - Configure workflow to trigger on push to main branch only
  - Add Rust toolchain setup (stable)
  - Add rust-cache action (Swatinem/rust-cache@v2) for dependency caching
  - Add step to run `cargo bench`
  - Add step to upload benchmark results as artifacts (target/criterion/)
  - Add optional step: Comment on PR if regression &gt;10% detected (document as future enhancement)
  - Test workflow by pushing to main branch and verifying CI runs benchmark job

- Task 5: Verify integration with existing CI/CD (AC: #15)
  - Run full CI suite: `cargo build &amp;&amp; cargo test &amp;&amp; cargo clippy &amp;&amp; cargo fmt --check &amp;&amp; cargo deny check`
  - Verify all checks pass
  - Push code to trigger GitHub Actions CI
  - Verify existing ci.yml workflow still passes all checks
  - Verify new benchmark.yml workflow runs separately and succeeds
</tasks>
  </story>

  <acceptanceCriteria>
1. `Cargo.toml` includes criterion as dev-dependency with html_reports feature
2. `Cargo.toml` includes [[bench]] section with name "rendering" and harness = false
3. `benches/rendering.rs` exists with three placeholder benchmarks
4. Placeholder benchmark: bench_braille_grid_creation exists
5. Placeholder benchmark: bench_grid_clear exists
6. Placeholder benchmark: bench_unicode_conversion exists
7. `cargo bench` runs successfully with no errors
8. `cargo bench` generates console output with timing results
9. `cargo bench` generates HTML reports in target/criterion/ directory
10. Benchmark reports show comparison against baseline (if previous run exists)
11. `.github/workflows/benchmark.yml` workflow file created
12. Benchmark workflow runs on main branch pushes
13. Benchmark workflow stores results as CI artifacts
14. Benchmark workflow configured to detect &gt;10% performance regressions
15. All existing CI checks (build, test, clippy, fmt, deny) continue to pass
</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>NFR-P1: Performance Above All Else</section>
        <snippet>Performance is make-or-break for dotmax. Target: &lt;25ms image rendering. Criterion.rs benchmarking with CI/CD regression testing. Comprehensive benchmark suite validates &lt;50ms renders and catches regressions immediately.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>NFR-P7: Competitive Benchmarking</section>
        <snippet>Continuous benchmarking against drawille, ascii-image-converter. Performance regression detection in CI pipeline. Public benchmark results for transparency.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>Decision Summary - Benchmarking</section>
        <snippet>criterion 0.7.0 for statistics-driven benchmarking with HTML reports. Performance regression detection in CI. Measure-first approach: no optimization without benchmark proof.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>ADR 0007: Measure-First Performance Optimization</section>
        <snippet>No optimization without benchmark proof. Use criterion for all performance work. Profile with flamegraph before optimizing. criterion tracks regressions automatically.</snippet>
      </doc>
      <doc>
        <path>docs/sprint-artifacts/tech-spec-epic-1.md</path>
        <title>Epic 1 Technical Specification</title>
        <section>Story 1.6 - Benchmarking Infrastructure</section>
        <snippet>Criterion.rs with placeholder benchmarks (grid creation, grid clear, unicode conversion). GitHub Actions workflow on main branch only. HTML reports in target/criterion/. &gt;10% regression detection threshold.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>Cargo.toml</path>
        <kind>manifest</kind>
        <symbol>[dev-dependencies], [[bench]]</symbol>
        <lines>32-33, TBD</lines>
        <reason>Story adds criterion dev-dependency and [[bench]] section for rendering benchmarks</reason>
      </artifact>
      <artifact>
        <path>.github/workflows/ci.yml</path>
        <kind>workflow</kind>
        <symbol>CI pipeline jobs</symbol>
        <lines>1-106</lines>
        <reason>Existing CI structure to reference for benchmark workflow pattern. Uses same actions (checkout, rust-toolchain, rust-cache)</reason>
      </artifact>
      <artifact>
        <path>src/lib.rs</path>
        <kind>library_root</kind>
        <symbol>lib.rs</symbol>
        <lines>1-25</lines>
        <reason>Core library file currently empty. Benchmarks will test placeholder operations until Epic 2 implements BrailleGrid</reason>
      </artifact>
    </code>
    <dependencies>
      <rust>
        <package name="criterion" version="0.7" scope="dev" features="html_reports">Benchmarking framework with statistical analysis and HTML report generation</package>
        <package name="tracing-subscriber" version="0.3" scope="dev">Logging support for development and testing</package>
      </rust>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>MSRV 1.70: All code must compile on Rust 1.70+ (enforced in CI)</constraint>
    <constraint>No nightly features: Criterion must work on stable Rust only</constraint>
    <constraint>Placeholder benchmarks only: Epic 1 has no functional code yet - benchmarks simulate future operations</constraint>
    <constraint>harness = false required: Criterion provides custom benchmark harness, must disable built-in harness</constraint>
    <constraint>Main branch only for benchmarks: Workflow triggers on push to main, not PRs (per Tech Spec Open Question #1)</constraint>
    <constraint>Cross-platform CI compatibility: Benchmark workflow must work on ubuntu-latest (same as existing CI)</constraint>
    <constraint>Binary size target: Core library must stay &lt;2MB (benchmarking infrastructure counts as dev dependency, doesn't affect library size)</constraint>
  </constraints>
  <interfaces>
    <interface>
      <name>Cargo bench command</name>
      <kind>CLI command</kind>
      <signature>cargo bench [--bench rendering]</signature>
      <path>Cargo CLI</path>
    </interface>
    <interface>
      <name>Criterion benchmark function</name>
      <kind>Function signature</kind>
      <signature>fn bench_name(c: &amp;mut Criterion) { c.bench_function("name", |b| { b.iter(|| { black_box(operation()) }) }) }</signature>
      <path>benches/rendering.rs</path>
    </interface>
    <interface>
      <name>GitHub Actions artifact upload</name>
      <kind>CI workflow action</kind>
      <signature>uses: actions/upload-artifact@v4 with: { name: "criterion-results", path: "target/criterion/" }</signature>
      <path>.github/workflows/benchmark.yml</path>
    </interface>
  </interfaces>
  <tests>
    <standards>
Epic 1 Story 1.6 is infrastructure-only with no unit tests required. Testing approach focuses on validation:

**Benchmark Validation**: Run `cargo bench` locally to verify (1) benchmarks compile without errors, (2) Criterion executes and shows timing output, (3) HTML reports generated in target/criterion/, (4) Second run shows baseline comparison with "change" metrics.

**CI Integration Testing**: Push to main branch to verify benchmark workflow runs successfully, artifacts uploaded correctly.

**Existing CI Compatibility**: Run full CI suite (`cargo build &amp;&amp; cargo test &amp;&amp; cargo clippy &amp;&amp; cargo fmt --check &amp;&amp; cargo deny check`) to ensure new benchmark infrastructure doesn't break existing checks.

**Acceptance Validation**: All 15 acceptance criteria must be verifiable through manual inspection or CI logs.
    </standards>
    <locations>
      <location>benches/rendering.rs - Placeholder benchmarks for Epic 1</location>
      <location>target/criterion/ - Generated HTML reports and baseline data</location>
      <location>.github/workflows/benchmark.yml - CI workflow for automated benchmark execution</location>
      <location>Future: benches/image_conversion.rs, benches/animation.rs (Epic 3, 6)</location>
    </locations>
    <ideas>
      <idea ac="1,2">Verify Cargo.toml has criterion dev-dependency with html_reports feature and [[bench]] section with harness=false</idea>
      <idea ac="3,4,5,6">Run `cargo bench` and verify console output shows timing for bench_braille_grid_creation, bench_grid_clear, bench_unicode_conversion</idea>
      <idea ac="7,8,9">Check cargo bench exits with status 0, console shows timing statistics, HTML reports exist at target/criterion/report/index.html</idea>
      <idea ac="10">Run cargo bench twice in sequence, verify second run shows "change" metrics in console output</idea>
      <idea ac="11,12,13">Push to main branch, verify benchmark.yml workflow runs in GitHub Actions, check artifacts tab for criterion-results.zip</idea>
      <idea ac="14">Review benchmark.yml for baseline comparison logic (Criterion provides this automatically via console output)</idea>
      <idea ac="15">Run full CI locally before commit: cargo build &amp;&amp; cargo test &amp;&amp; cargo clippy &amp;&amp; cargo fmt --check &amp;&amp; cargo deny check</idea>
    </ideas>
  </tests>
</story-context>
